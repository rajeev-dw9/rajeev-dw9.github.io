<!DOCTYPE html>
<html lang="en">
<html>

<head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KCQFW8ML');</script>
<!-- End Google Tag Manager -->



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Rajeev Ranjan Dwivedi | Publications</title>
<meta name="description" content="Building fair AI, one line of code at a time.">
<meta name="keywords" content="PhD, Rajeev, Rajeev Ranjan Dwivedi, Rajeev Dwivedi, IISER Bhopal, IISER, Bhopal, Bias, Fairness in AI,Vision, Langauge, Google Scholar">


<!-- Open Graph -->

<meta property="og:site_name" content="Building fair AI, one line of code at a time.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="Rajeev Ranjan Dwivedi" />
<meta property="og:url" content="https://rajeev-dw9.github.io/" />
<meta property="og:description" content="Publications" />
<meta property="og:image"
  content="https:///rajeev-dw9.github.io/assets/img/prof_pic.png" />
<meta name="thumbnail"
  content="https:///rajeev-dw9.github.io/assets/img/prof_pic.png" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
  rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
  integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
  integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css"
  href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet"
  href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






<script data-goatcounter="https://rrd.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://rajeev-dw9.github.io/">
       <span class="font-weight-bold">Rajeev </span>Ranjan Dwivedi
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->

          
          <!-- Other pages -->

          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          <!-- CV -->
          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/Rajeev_Resume_GS.pdf", target="_blank">
              cv
            </a>
          </li>
          <!-- Resume -->

          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">Publications</h1>
          <!-- <p class="post-description">* Will be updating soon...</p> -->
        </header>
    
        <article>
          <div class="publications">
    <!-- Common CSS for standardized image shapes -->


        <!-- Publication Template -->
        <h2 class="year">2025</h2>


        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/gmbm.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">Multi-Attribute Bias Mitigation via Representation Learning</div>
                <div class="author">
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Ankur Kumar,
                  Vinod K Kurmi
                </div>
                <div class="periodical">
                  <em>European Conference on Artificial Intelligence (ECAI) 2025</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://visdomlab.github.io/GMBM/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">ECAI '25</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Real-world images frequently exhibit multiple overlapping biases, including textures, watermarks, gendered makeup, scene-object pairings, etc. These biases collectively impair the performance of modern vision models, undermining both their robustness and fairness. Addressing these biases individually proves inadequate, as mitigating one bias often permits or intensifies others.  We tackle this multi-bias problem with Generalized Multi-Bias Mitigation (GMBM), a lean two-stage framework that needs group labels only while training and minimizes bias at test time. First, Adaptive Bias-Integrated Learning (ABIL) deliberately identifies the influence of known shortcuts by training encoders for each attribute and integrating them with the main backbone, compelling the classifier to explicitly recognize these biases. Then Gradient-Suppression Fine-Tuning prunes those very bias directions from the backbone’s gradients, leaving a single compact network that ignores all the shortcuts it just learned to recognize. Moreover we find that existing bias metrics break under subgroup imbalance and train–test distribution shifts, so we introduce Scaled Bias Amplification (SBA): a test-time measure that disentangles model-induced bias amplification from distributional differences. We validate GMBM on FB-CMNIST, CelebA, and COCO, where we boost worst-group accuracy, halve multi-attribute bias amplification, and set a new low in SBA—even as bias complexity and distribution shifts intensify—making GMBM the first practical, end-to-end multi-bias solution for visual recognition.</p>
                </div>
              </div>
            </div>
          </li>
        </ol>




        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/grl.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">Equitable Dermatology: Adversarial and Spectral Techniques for Fair Skin Lesion Classification</div>
                <div class="author">
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Mohammedkaif Rafiq Kalagond,
                  Monika Sharma (TCS Research),
                  Amit Sangroya (TCS Research),
                </div>
                <div class="periodical">
                  <em>Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP) 2025</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">ICVGIP'25</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p> Deep learning models for skin‑lesion classification often underperform on darker skin tones, leading to diagnostic inequity. We propose a unified framework that combines adversarial debiasing via a Gradient Reversal Layer to remove skin‑tone information from feature representations and an Elastic Representation (ElRep) regularizer to learn robust, disentangled lesion features. By jointly optimizing lesion classification and skin‑tone prediction losses, our composite objective encourages the encoder to focus on lesion morphology rather than background. Evaluated on the large‑scale Fitzpatrick‑17k and ISIC2020 datasets, our approach achieves significant improvement in overall accuracy and worst‑group accuracy over standard empirical risk minimization, while maintaining fast and stable convergence. Qualitative t‑SNE and Grad‑CAM analyses confirm that our model learns lesion‑focused embeddings across all skin types, offering a practical, reproducible path toward more equitable AI‑driven dermatology.</p>
                </div>
              </div>
            </div>
          </li>
        </ol>
        <!-- Publication Template -->

        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/gcl.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment</div>
                <div class="author">
                  Rini Smita Thakur,
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Vinod K Kurmi
                </div>
                <div class="periodical">
                  <em>British Machine Vision Conference (BMVC), 2025</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://www.arxiv.org/abs/2509.10134" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">BMVC '25</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Accurate segmentation of the optic disc and cup is critical for the early diagnosis and management of ocular diseases such as glaucoma. However, segmentation models trained on one dataset often suffer significant performance degradation when applied to target data acquired under different imaging protocols or conditions. To address this challenge, we propose Grad-CL, a novel source-free domain adaptation framework that leverages a pre-trained source model and unlabeled target data to robustly adapt segmentation performance without requiring access to the original source data. Grad-CL combines a gradient-guided pseudolabel refinement module with a cosine similarity–based contrastive learning strategy. In the first stage, salient class-specific features are extracted via a gradient-based mechanism, enabling more accurate uncertainty quantification and robust prototype estimation for refining noisy pseudolabels. In the second stage, a contrastive loss based on cosine similarity is employed to explicitly enforce inter-class separability between the gradient-informed features of the optic cup and disc. Extensive experiments on challenging cross-domain fundus imaging datasets demonstrate that Grad-CL outperforms state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and improved boundary delineation.</p>
                </div>
              </div>
            </div>
          </li>
        </ol>
        <!-- Publication Template -->


        <!-- Publication Template -->
        <h2 class="year">2024</h2>
        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/cosfairnet.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">CosFairNet: A Parameter-Space based Approach for Bias Free Learning</div>
                <div class="author">
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Priyadarshni Kumari (Sony AI),
                  Vinod K Kurmi
                </div>
                <div class="periodical">
                  <em>British Machine Vision Conference (BMVC), (2024)</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://bmvc2024.org/proceedings/738/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">BMVC '24</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Deep neural networks trained on biased data often inadvertently learn unintended inference rules, particularly when labels are strongly correlated with biased features. Existing bias mitigation methods typically involve either a) predefining bias types and enforcing them as prior knowledge or b) reweighting training samples to emphasize bias-conflicting samples over bias-aligned samples. However, both strategies address bias indirectly in the feature or sample space, with no control over learned weights, making it difficult to control the bias propagation across different layers. Based on this observation, we introduce a novel approach to address bias directly in the model's parameter space, preventing its propagation across layers. Our method involves training two models: a bias model for biased features and a debias model for unbiased details, guided by the bias model. We enforce dissimilarity in the debias model's later layers and similarity in its initial layers with the bias model, ensuring it learns unbiased low-level features without adopting biased high-level abstractions. By incorporating this explicit constraint during training, our approach shows enhanced classification accuracy and debiasing effectiveness across various synthetic and real-world datasets of different sizes. Moreover, the proposed method demonstrates robustness across different bias types and percentages of biased samples in the training data.</p>
                </div>
              </div>
            </div>
          </li>
        </ol>
        <!-- Publication Template -->


        <!-- Publication Template -->
        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/Het_GP.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">Quantifying Uncertainty in Neural Networks through Residuals</div>
                <div class="author">
                  Udhbav Dalavi,
                  Rini Smita Thakur,
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Vinod K Kurmi
                </div>
                <div class="periodical">
                  <em> 33rd ACM International Conference on Information and Knowledge Management (CIKM), (2024)</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/10.1145/3627673.3679983" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">CIKM’24</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>   Regression models are of fundamental importance in explicitly explaining the response variable in terms of covariates. Heteroscedasticity is common in most real-world scenarios and is hard to model due to its randomness. The Gaussian process generally captures epistemic (model) uncertainty but fails to capture heteroscedastic aleatoric uncertainty. The framework of Heteroscedastic Gaussian Process (HetGP) inherently captures both epistemic and aleatoric by placing independent Gaussian processes on both mean function and error term.  In this work, we propose the posthoc HetGP on the residuals of the trained neural network to obtain both epistemic and aleatoric uncertainty. The advantage of posthoc HetGP on residuals is that it can be extended to any type of model, since the model is assumed to be black-box that gives point predictions. We demonstrate our approach through simulation studies on UCI regression datasets. </p>
                </div>
              </div>
            </div>
          </li>
        </ol>
        <!-- Publication Template -->


        <!-- h2 class="year">2023</h2 -->

        <!-- Publication Template -->
        <h2 class="year">2023</h2>
        <ol class="bibliography">
          <li>
            <div class="row">
              <div class="col-sm-3 abbr">
                <div class="float-left">
                  <img class="img-fluid publication-image" src="/assets/img/grb.png" alt="Publication Image" />
                </div> 
              </div>
              <div class="col-sm-7">
                <div class="title">Predicting Missing Light Curves of Gamma-Ray Bursts with Bidirectional-LSTM: An Approach for Enhanced Analysis</div>
                <div class="author">
                  Shashwat Sourav,
                  Amit Shukla,
                  <em>Rajeev Ranjan Dwivedi</em>,
                  Kartikey Singh
                </div>
                <div class="periodical">
                  <em>Accepted at SPAICE,24 ECSAT, UK</em>
                </div>
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://arxiv.org/pdf/2310.02602" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                  <a class="conference btn-sm z-depth-0" role="button">arXiv '23</a>
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Gamma-ray bursts (GRB) are powerful transient events that emit a large output of gamma rays within a few seconds. Studying these short bursts is vital for cosmological research since they originate from sources observed at large redshifts. To effectively carry out these studies, it is crucial to establish a correlation between the observable features of GRBs while reducing their uncertainty. For these reasons, a comprehensive description of the general GRB light curve (LC) would be crucial for the studies. However, unevenly spaced observations and significant gaps in the LC, which are primarily unavoidable for various reasons, make it difficult to characterize GRBs. Therefore, the general classification of GRB LCs remains challenging. In this study, we present a novel approach to reconstruct gamma-ray burst (GRB) light curves using bidirectional Long Short-Term Memory (BiLSTM). Experimental results show that the BiLSTM approach performs better than traditional methods and produces smoother and more convincing reconstructions for GRBs.</p>
                </div>
              </div>
            </div>
          </li>
        </ol>
        <!-- Publication Template -->





          </div>
        </article>
      </div>
    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Rajeev Ranjan Dwivedi.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
